{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matlab.engine\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time configurations\n",
    "num_workers = 4\n",
    "create_amount = 200\n",
    "batch_size = 20\n",
    "IMG_X = 200\n",
    "IMG_Y = 300\n",
    "depth = 40e-3\n",
    "dz = (80e-3 - 10e-3)/300\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = []\n",
    "for i in range(num_workers):\n",
    "    eng.append(matlab.engine.start_matlab())\n",
    "    eng[i].init_field(nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_line(depth,dz,im):\n",
    "    return im[:,(depth-10e-3)/dz,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lines(eng,delays):\n",
    "    delays = torch.reshape(delays,(-1,128))\n",
    "    delays = matlab.double(delays.tolist())\n",
    "    batch  = eng.create_new_line(delays)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images(eng,delays):\n",
    "    delays = torch.reshape(delays,(-1,128))\n",
    "    delays = matlab.double(delays.tolist())\n",
    "    batch  = eng.create_new_image(delays)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_minimal_distance(a,b,minimal):\n",
    "    if isinstance(a,list):\n",
    "        for loc in a:\n",
    "            if abs(loc - b) < minimal:\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        if a == 0:\n",
    "            return True\n",
    "        return abs(a - b) > minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = (15e-3 + 15e-3)/IMG_X\n",
    "c = 1490\n",
    "f0 = 4.464e6\n",
    "D = 0.218e-3 * 128\n",
    "minimal_distance = (1.206 * (c/f0) * 40e-3) / D\n",
    "minimum = minimal_distance / dx\n",
    "f = np.vectorize(check_minimal_distance)\n",
    "x = np.arange(-10*minimal_distance,10*minimal_distance,dx)\n",
    "sinc = np.sinc(1045.61 * x) #number computed numerically for the width of the sinc\n",
    "plt.plot(x,sinc)\n",
    "print(sinc.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patterns_1d(amount,N,seq_length = 50):\n",
    "    patterns = np.zeros((amount,N))\n",
    "    number_in_each = np.random.randint(1,2,amount)\n",
    "    dx = (15e-3 + 15e-3)/N\n",
    "    c = 1490\n",
    "    f0 = 4.464e6\n",
    "    D = 0.218e-3 * 128\n",
    "    minimal_distance = (1.206 * (c/f0) * 40e-3) / D\n",
    "    minimum = minimal_distance / dx\n",
    "    f = np.vectorize(check_minimal_distance)\n",
    "    #x = np.arange(-10*minimal_distance,10*minimal_distance,dx)\n",
    "    x = np.linspace(-30e-3,30e-3,N * 2)\n",
    "    sinc = np.sinc(1045.61 * x) #number computed numerically for the width of the sinc\n",
    "    #sinc = np.sinc()\n",
    "    sinc_len = len(sinc)\n",
    "    for i in range(amount):\n",
    "        j = 0\n",
    "        actual = []\n",
    "        while j < number_in_each[i]:\n",
    "            point = np.random.randint(0,N,1)\n",
    "            val = point[0]\n",
    "            if check_minimal_distance(actual,point,minimum):\n",
    "                actual.append(val)\n",
    "                j += 1\n",
    "        #patterns[i,positions] = 1\n",
    "        for position in actual:\n",
    "            shifted_sinc = np.roll(sinc,position - 100)\n",
    "            patterns[i,:] += shifted_sinc[round(N/2):round(3*N/2)]\n",
    "            #position = positions[k,0]\n",
    "            #lower = max(0,position - round(sinc_len/2))\n",
    "            #upper = min(IMG_X,position+round(sinc_len/2) - 1)\n",
    "            #actual_len = upper - lower + 1\n",
    "            #print(lower,upper,actual_len)\n",
    "            #if lower == 0:\n",
    "            #    to_apply = sinc[-actual_len:]\n",
    "            #elif upper == IMG_X:\n",
    "            #      to_apply = sinc[:actual_len - 1]\n",
    "            #else:\n",
    "            #      to_apply = sinc\n",
    "            #patterns[i,lower:upper] += to_apply\n",
    "    return torch.from_numpy(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-30e-3,30e-3,IMG_X * 2)\n",
    "sinc = np.sinc(1045.61 * x) #number computed numerically for the width of the sinc            shifted_sinc = np.roll(sinc,round(N/2 + position))\n",
    "shifted_sinc = np.roll(sinc,150)\n",
    "plt.plot(x,shifted_sinc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = create_patterns_1d(16,IMG_X)\n",
    "x = np.linspace(-15e-3,15e-3,IMG_X)\n",
    "fig, ax = plt.subplots(4,4)\n",
    "for i,pattern in enumerate(patterns):\n",
    "    ax[i // 4][i % 4].plot(x,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseDataSet(Dataset):\n",
    "    def __init__(self,indexes,source_data,seq_length = 50 ,csv_file = \"C:/Users/drors/Desktop/code for thesis/data.csv\",transforms = None):\n",
    "        #self.data = pd.read_csv(csv_file,header = None, index_col = 0,skiprows = )\n",
    "        self.data = source_data.iloc[indexes,:].copy()\n",
    "        self.csv = csv_file\n",
    "        self.measure = nn.CosineSimilarity()\n",
    "        self.seq_length = seq_length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.data.iloc[idx,:IMG_X] * 1e6\n",
    "        y = self.data.iloc[idx,IMG_X:] * 1e6\n",
    "        return torch.tensor(x.values).reshape(-1,self.seq_length),torch.tensor(y.values)\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "    def add_batch(self,new_data):\n",
    "        #similarity = cosine_similarity(new_data,self.data)\n",
    "        distance = distance_matrix(new_data,self.data)\n",
    "        mask = np.all(distance > 1e-5, axis = 1)\n",
    "\n",
    "        #print(new_data.shape,self.data.shape,similarity.shape,mask.shape)\n",
    "        new_data = new_data.loc[mask,:]\n",
    "        #new_df = pd.DataFrame(new_data,header = None)\n",
    "        self.data = self.data.append(new_data,ignore_index = True)\n",
    "        if len(self.data) > 10000:\n",
    "            self.data = self.data.iloc[(len(self.data) - 10000):,:] \n",
    "        print(len(self.data))\n",
    "        #new_df.to_csv(self.csv,mode = 'a', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetManager():\n",
    "    def __init__(self,eng,seq_length = 50,csv_file = \"C:/Users/drors/Desktop/code for thesis/data.csv\",orig = \"C:/Users/drors/Desktop/code for thesis/data original.csv\"):\n",
    "        self.eng = eng\n",
    "        self.csv_file = csv_file\n",
    "        self.orig = orig\n",
    "        self.train,self.val,self.test = self.create_datasets()\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def create_datasets(self):\n",
    "        data = pd.read_csv(self.orig,header = None,index_col = None, engine = 'python')\n",
    "        n = len(data)\n",
    "        perm = np.random.permutation(n)\n",
    "        train_idx = perm[:round(0.8*n)]\n",
    "        val_idx = perm[round(0.8*n):round(0.9*n)]\n",
    "        test_idx = perm[round(0.9*n):]\n",
    "        train_dataset = baseDataSet(train_idx,data)\n",
    "        val_dataset = baseDataSet(val_idx,data)\n",
    "        test_dataset = baseDataSet(test_idx,data)\n",
    "        return train_dataset,val_dataset,test_dataset\n",
    "    def get_datasets(self):\n",
    "        return self.train,self.val,self.test\n",
    "    def reset(self):\n",
    "        data = pd.read_csv(self.orig,header = None,index_col = None, engine = 'python')\n",
    "        n = len(data)\n",
    "        perm = np.random.permutation(n)\n",
    "        train_idx = perm[:round(0.8*n)]\n",
    "        val_idx = perm[round(0.8*n):round(0.9*n)]\n",
    "        test_idx = perm[round(0.9*n):]\n",
    "        train_dataset = baseDataSet(train_idx,data)\n",
    "        val_dataset = baseDataSet(val_idx,data)\n",
    "        test_dataset = baseDataSet(test_idx,data)\n",
    "        self.train,self.val,self.test =  train_dataset,val_dataset,test_dataset\n",
    "        copyfile(self.orig,self.csv_file)\n",
    "        \n",
    "    def add_batch_to_data(self,batch, mode = 'create'):\n",
    "        new_df = pd.DataFrame(batch).astype('float')\n",
    "        new_df.to_csv(self.csv_file,mode = 'a', header = None,index = False)\n",
    "        n = len(new_df)\n",
    "        perm = np.random.permutation(n)\n",
    "        train_idx = perm[:round(0.8*n)]\n",
    "        val_idx = perm[round(0.8*n):round(0.9*n)]\n",
    "        test_idx = perm[round(0.9*n):]\n",
    "        if mode == 'create':\n",
    "            self.train = baseDataSet(train_idx,new_df) # .add_batch(new_df.iloc[train_idx,:])\n",
    "            self.val = baseDataSet(val_idx,new_df)# .add_batch(new_df.iloc[val_idx,:])\n",
    "            self.test = baseDataSet(test_idx,new_df)# .add_batch(new_df.iloc[test_idx,:])\n",
    "        else:\n",
    "            self.train.add_batch(new_df.iloc[train_idx,:])\n",
    "            self.val.add_batch(new_df.iloc[val_idx,:])\n",
    "            self.test.add_batch(new_df.iloc[test_idx,:])\n",
    "    def generate_base_dataset(self,amount = create_amount, mode = 'create'):\n",
    "        amount *= 5\n",
    "        delays = torch.rand(amount,128) * 1e-5\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            new_delay = torch.reshape(delays,(len(self.eng),-1))\n",
    "            new_delay = new_delay * 1e-6\n",
    "            results = executor.map(create_lines,self.eng,new_delay)\n",
    "        res = np.zeros((amount,IMG_X))\n",
    "        size_eng = amount / len(self.eng)\n",
    "        for i,line in enumerate(results):\n",
    "            res[round(size_eng * i):round(size_eng *(i+1)),:] = line\n",
    "#        print(np.fromiter(results,matlab.double))\n",
    "        self.add_batch_to_data(torch.cat((torch.tensor(res),delays),1),mode)\n",
    "        torch.cat((torch.tensor(res),delays),1)\n",
    "    def create_batch(self,net,amount = create_amount):\n",
    "        patterns = create_patterns_1d(amount,IMG_X)\n",
    "        net.eval()\n",
    "        if isinstance(net,lstmModel):\n",
    "            h = net.init_hidden(amount)\n",
    "            patterns_to_send = patterns.reshape(amount,-1,self.seq_length)\n",
    "            delays,_ = net(patterns_to_send.float(),h)\n",
    "        else:\n",
    "            #patterns_to_send = patterns.reshape(amount,-1,self.seq_length)\n",
    "            delays = net(patterns.float())\n",
    "        \n",
    "        while np.any(delays.detach().numpy() > 1e-5):\n",
    "            delays *= 1e-1\n",
    "        if torch.any(torch.isnan(delays)):\n",
    "            print(delays)\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            new_delay = torch.reshape(delays,(len(self.eng),-1))\n",
    "            new_delay = new_delay * 1e-6\n",
    "            results = executor.map(create_lines,self.eng,new_delay)\n",
    "        res = np.zeros((amount,IMG_X))\n",
    "        size_eng = amount / len(self.eng)\n",
    "        for i,line in enumerate(results):\n",
    "            res[round(size_eng * i):round(size_eng *(i+1)),:] = line\n",
    "#        print(np.fromiter(results,matlab.double))\n",
    "        return torch.from_numpy(res),delays,patterns\n",
    "\n",
    "    def create_pressure_batch(self,delays,amount = 8):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            delays = delays * 1e-6\n",
    "            new_delay = torch.reshape(delays,(len(self.eng),-1))\n",
    "            results = executor.map(create_images,self.eng,new_delay)\n",
    "        res = np.zeros((amount,IMG_Y,IMG_X))\n",
    "        size_eng = amount / len(self.eng)\n",
    "        for i,line in enumerate(results):\n",
    "            res[round(size_eng * i):round(size_eng *(i+1)),:,:] = line\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(net,data_manager,amount = 8):\n",
    "    results,delays,orig_patterns = data_manager.create_batch(net,amount)\n",
    "    images = data_manager.create_pressure_batch(delays)\n",
    "    x = np.linspace(-15e-3,15e-3,IMG_X)\n",
    "    z = np.linspace(10e-3,80e-3,300)\n",
    "    if amount == 4:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (10,5))\n",
    "        ax2.plot(x *1e3,results[0,:])\n",
    "        ax2.set_title('result from net')\n",
    "        ax1.plot(x *1e3,orig_patterns[0,:])\n",
    "        ax1.set_title('expected')\n",
    "        ax3.imshow(np.rot90(images[0,:,:],4),cmap = 'hot',extent = [-15,15,80,10])\n",
    "    else:\n",
    "        for i in range(amount):\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (10,5))\n",
    "            ax2.plot(x *1e3,results[i,:])\n",
    "            ax2.set_title('result from net')\n",
    "            ax1.plot(x *1e3,orig_patterns[i,:])\n",
    "            ax1.set_title('expected')\n",
    "            ax3.imshow(np.rot90(images[i,:,:],4),cmap = 'hot',extent = [-15,15,80,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataManager = datasetManager(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstmModel(nn.Module):\n",
    "    def __init__(self,drop = 0.6,in_size = IMG_X,out_size = 128, n_layers = 2,seq_length = 50):\n",
    "        super(lstmModel,self).__init__()\n",
    "        self.in_size = seq_length\n",
    "        self.out_size = out_size\n",
    "        self.n_layers = n_layers\n",
    "        self.seq_length = seq_length\n",
    "        self.rnn = nn.LSTM(self.in_size, out_size, n_layers,\n",
    "                           dropout = drop, batch_first=True, bidirectional = True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.fc = nn.Linear(out_size,out_size)\n",
    "        self.fc2 = nn.Linear(2* out_size,out_size)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out = self.dropout(x)\n",
    "        out,hidden = self.rnn(out,hidden)\n",
    "        out = out.contiguous().view(-1, self.out_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = out[:,-2 * self.out_size:]\n",
    "        out = self.fc2(out)\n",
    "        #out = out * -1e-5\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers * 2, batch_size, self.out_size).zero_().to(device),\n",
    "                      weight.new(self.n_layers * 2, batch_size, self.out_size).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLstm(net,opt,dataManager,schedular,n_epochs = 30,batch_size = 20):\n",
    "    datasets = dataManager.get_datasets()\n",
    "    train = DataLoader(datasets[0],batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val = DataLoader(datasets[1],batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    test = DataLoader(datasets[2],batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    net.to(device)\n",
    "    train_loss_total = []\n",
    "    val_loss_total = []\n",
    "    test_loss_total = []\n",
    "    criterion = nn.L1Loss() #will probably have outliers, L1 is more robust\n",
    "    #criterion = nn.MSELoss()\n",
    "    #criterion = nn.CosineSimilarity()\n",
    "    for i in range(n_epochs):\n",
    "        training_loss = 0\n",
    "        val_loss = 0\n",
    "        net.train()\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for x,y in train:\n",
    "            h = tuple([e.data for e in h])\n",
    "            x.to(device),y.to(device)\n",
    "            opt.zero_grad()\n",
    "            output, h  = net(x.float(),h)\n",
    "            loss = criterion(output,y.float())\n",
    "            loss.mean().backward()\n",
    "            training_loss += loss.mean().item()\n",
    "            #nn.utils.clip_grad_norm_(net.parameters(), 1) \n",
    "            opt.step()\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            h = net.init_hidden(batch_size)\n",
    "            for x,y in val:\n",
    "                h = tuple([e.data for e in h])\n",
    "                x.to(device),y.to(device)\n",
    "                output, h  = net(x.float(),h)\n",
    "                loss = criterion(output,y.float())\n",
    "                val_loss += loss.mean().item()\n",
    "        training_loss = training_loss / len(train)\n",
    "        if len(val) == 0:\n",
    "            val_loss = 'no values'\n",
    "        else:\n",
    "            val_loss = val_loss / len(val)\n",
    "        print(f'epoch num: {i}, train loss: {training_loss}, validation loss:{val_loss}, length of train {len(train)}')\n",
    "        schedular.step()\n",
    "        results,delays,orig_patterns = dataManager.create_batch(net,amount = 100)\n",
    "        dataManager.add_batch_to_data(torch.cat((results,delays),1), mode = 'add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_model(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_size = IMG_X,out_size = 128,drop = 0.2):\n",
    "        super(basic_model, self).__init__()\n",
    "        self.drop = drop\n",
    "        self.fc1 = nn.Sequential(nn.BatchNorm1d(in_size),\n",
    "                                 nn.Linear(in_size,150),\n",
    "                                #nn.BatchNorm1d(180),\n",
    "                                nn.Dropout(p=drop),\n",
    "                                #nn.ReLU(),\n",
    "                                #nn.Linear(180,150),\n",
    "                                #nn.BatchNorm1d(150),\n",
    "                                #nn.Dropout(p=0.2),\n",
    "                                nn.ReLU())\n",
    "        self.fc2 = nn.Linear(150,out_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size,-1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        #out = out *-1e-5\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(net,opt,dataManager,schedular,n_epochs = 30,batch_size = 20):\n",
    "    datasets = dataManager.get_datasets()\n",
    "    train = DataLoader(datasets[0],batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val = DataLoader(datasets[1],batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    test = DataLoader(datasets[2],batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    net.to(device)\n",
    "    train_loss_total = []\n",
    "    val_loss_total = []\n",
    "    test_loss_total = []\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss() #will probably have outliers, L1 is more robust\n",
    "    #criterion = nn.CosineSimilarity()\n",
    "    for i in range(n_epochs):\n",
    "        training_loss = 0\n",
    "        val_loss = 0\n",
    "        net.train()\n",
    "        for x,y in train:\n",
    "            x.to(device),y.to(device)\n",
    "            opt.zero_grad()\n",
    "            output = net(x.float())\n",
    "            loss = criterion(output,y.float())\n",
    "            loss.mean().backward()\n",
    "            training_loss += loss.mean().item()\n",
    "            opt.step()\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for x,y in val:\n",
    "                x.to(device),y.to(device)\n",
    "                output = net(x.float())\n",
    "                loss = criterion(output,y.float())\n",
    "                val_loss += loss.mean().item()\n",
    "        training_loss = training_loss / len(train)\n",
    "        if len(val) == 0:\n",
    "            val_loss = 'no values'\n",
    "        else:\n",
    "            val_loss = val_loss / len(val)\n",
    "        print(f'epoch num: {i}, train loss: {training_loss}, validation loss:{val_loss}, length of train {len(train)}')\n",
    "        schedular.step()\n",
    "        results,delays,orig_patterns = dataManager.create_batch(net,amount = 100)\n",
    "        dataManager.add_batch_to_data(torch.cat((results,delays),1), mode = 'add')\n",
    "#        if i%3 == 0:\n",
    "#            print(f'testing iteration number: {i}')\n",
    "#            test_batch(net,dataManager,amount = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataManager.reset()\n",
    "dataManager.generate_base_dataset()\n",
    "dataManager.generate_base_dataset(mode = 'add')\n",
    "dataManager.generate_base_dataset(mode = 'add')\n",
    "#dataManager.generate_base_dataset(mode = 'add')\n",
    "#dataManager.generate_base_dataset(mode = 'add')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = lstmModel(n_layers = 2)\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001, momentum = 0.9, weight_decay = 50)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "trainLstm(net,optimizer,dataManager,scheduler,n_epochs = 6)\n",
    "test_batch(net,dataManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = basic_model(drop = 0.6)\n",
    "print(net.fc1[0].weight,net.fc1[0].bias)\n",
    "optimizer = optim.SGD(net.parameters(),lr = 1, momentum = 0.9, weight_decay = 5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "trainModel(net,optimizer,dataManager,scheduler,n_epochs = 20)\n",
    "test_batch(net,dataManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.fc1[0].weight,net.fc1[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = basic_model(drop = 0.6)\n",
    "print('trying to create new batch')\n",
    "results,delays,orig_patterns = dataManager.create_batch(net)\n",
    "print('created new batch, trying to add to data')\n",
    "dataManager.add_batch_to_data(torch.cat((results,delays),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch(net,dataManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net,net.rnn.weight_ih_l[0] , net.rnn.weight_hh_l[0],net.rnn.bias_ih_l[0],net.rnn.bias_hh_l[0],self.rnn.weight_hr_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataManager.get_datasets()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataManager.get_datasets()\n",
    "train = datasets[0]\n",
    "x_1,y_1 = train[20]\n",
    "x_2,y_2 = train[5]\n",
    "x_1 = x_1.reshape(1,-1,50).float()\n",
    "x_2 = x_2.reshape(1,-1,50).float()\n",
    "criterion = nn.MSELoss()\n",
    "h_1 = net.init_hidden(1)\n",
    "h_2 = net.init_hidden(1)\n",
    "print(x_1.size())\n",
    "print(criterion(x_1,x_2))\n",
    "print(criterion(net(x_1,h_1)[0],net(x_2,h_2)[0]))\n",
    "print(criterion(y_1,y_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch(net,dataManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "copied from matlab, if want to use need to adjust\n",
    "\n",
    "def calc_delay(focus,N_elements = 128, c = 1490, pitch = 0.218e-3):\n",
    "    first = np.norm(focus)\n",
    "    centers = np.arange(-num_elements/2+1,num_elements/2)\n",
    "    centers = centers * pitch\n",
    "    centers = [centers zeros(length(centers),2)];\n",
    "    second = vecnorm(centers - focus,2,2)\n",
    "    Delay = (first - second)/c\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd06baff4032bbef77a06aa1ff8dc383ac397044626828838ab42d27ce151c9c4f1",
   "display_name": "Python 3.8.8 64-bit ('thesis': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}